{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIG DATA ASSIGNMENT # 2\n",
    "### Haroon Salim 21i-1663  \n",
    "### Immad Shahid Qureshi 21i-1664  \n",
    "### Ahmad Luqman 22i-2018  \n",
    "### DS (D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEARCH ENGINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading CSV using PYARROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"enwiki-20170820.csv\"\n",
    "\n",
    "# Define schema if you know the schema beforehand\n",
    "# schema = pv.read_csv(file_path).schema\n",
    "\n",
    "# Define options to control the CSV parsing process\n",
    "# You may adjust these options based on your dataset's characteristics\n",
    "options = pv.ParseOptions(delimiter=',', header=True)\n",
    "\n",
    "# Define read options\n",
    "# We'll use lazy loading to minimize memory usage\n",
    "read_options = pv.ReadOptions(use_threads=True)\n",
    "\n",
    "# Load the CSV file lazily using PyArrow\n",
    "# The result is a Table object that represents the CSV data\n",
    "table = pv.read_csv(file_path, parse_options=options, read_options=read_options)\n",
    "\n",
    "# Convert the Table to a Parquet file for efficient storage and further processing\n",
    "# You can specify the output file path where the Parquet file will be saved\n",
    "output_path = \"enwiki-20170820.parquet\"\n",
    "pq.write_table(table, output_path)\n",
    "\n",
    "# Optional: If you want to inspect the schema of the loaded data\n",
    "print(\"Schema:\")\n",
    "print(table.schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema\n",
    "print(table.schema)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(table.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "table = table.distinct()\n",
    "\n",
    "# Handle missing values\n",
    "# For demonstration, let's assume we want to fill missing values with a default value\n",
    "# Replace null values in each column with a default value (e.g., \"\")\n",
    "for column in table.schema.names:\n",
    "    table = table.set_column(column, table.column(column).fillna(\"\"))\n",
    "\n",
    "\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "preprocessed_file_path = \"preprocessed_enwiki-20170820.parquet\"\n",
    "pq.write_table(table, preprocessed_file_path)\n",
    "\n",
    "# Optional: Display the schema and preview of the preprocessed dataset\n",
    "print(\"Preprocessed Schema:\")\n",
    "print(table.schema)\n",
    "print(\"Preview of the Preprocessed Dataset:\")\n",
    "print(table.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Data Cleaning and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.set_column(\"column_name\", table.column(\"column_name\").cast(pa.string()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.add_column(\"new_feature\", table.column(\"existing_feature_1\") + table.column(\"existing_feature_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(table.column(\"numeric_column\").to_pandas(), bins=20)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Numeric Column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "correlation_coefficient, p_value = pearsonr(table.column(\"column_1\").to_numpy(), table.column(\"column_2\").to_numpy())\n",
    "print(\"Pearson correlation coefficient:\", correlation_coefficient)\n",
    "print(\"P-value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example: Standardize numeric features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(table.select([\"numeric_feature_1\", \"numeric_feature_2\"]).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Splitting the dataset into training and test sets\n",
    "train_set, test_set = train_test_split(table.to_pandas(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model building and analysis\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# simple linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(train_set[[\"feature_1\", \"feature_2\"]], train_set[\"target_variable\"])\n",
    "predictions = model.predict(test_set[[\"feature_1\", \"feature_2\"]])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
