{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIG DATA ASSIGNMENT # 2\n",
    "### Haroon Salim 21i-1663  \n",
    "### Immad Shahid Qureshi 21i-1664  \n",
    "### Ahmad Luqman 22i-2018  \n",
    "### DS (D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEARCH ENGINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"enwiki-20170820.csv\"\n",
    "\n",
    "# Define schema if you know the schema beforehand\n",
    "# schema = pv.read_csv(file_path).schema\n",
    "\n",
    "# Define options to control the CSV parsing process\n",
    "# You may adjust these options based on your dataset's characteristics\n",
    "options = pv.ParseOptions(delimiter=',', header=True)\n",
    "\n",
    "# Define read options\n",
    "# We'll use lazy loading to minimize memory usage\n",
    "read_options = pv.ReadOptions(use_threads=True)\n",
    "\n",
    "# Load the CSV file lazily using PyArrow\n",
    "# The result is a Table object that represents the CSV data\n",
    "table = pv.read_csv(file_path, parse_options=options, read_options=read_options)\n",
    "\n",
    "# Convert the Table to a Parquet file for efficient storage and further processing\n",
    "# You can specify the output file path where the Parquet file will be saved\n",
    "output_path = \"enwiki-20170820.parquet\"\n",
    "pq.write_table(table, output_path)\n",
    "\n",
    "# Optional: If you want to inspect the schema of the loaded data\n",
    "print(\"Schema:\")\n",
    "print(table.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the schema\n",
    "print(table.schema)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(table.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "table = table.distinct()\n",
    "\n",
    "# Handle missing values\n",
    "# For demonstration, let's assume we want to fill missing values with a default value\n",
    "# Replace null values in each column with a default value (e.g., \"\")\n",
    "for column in table.schema.names:\n",
    "    table = table.set_column(column, table.column(column).fillna(\"\"))\n",
    "\n",
    "# Additional preprocessing steps can be added here, such as tokenization, normalization, etc.\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "preprocessed_file_path = \"preprocessed_enwiki-20170820.parquet\"\n",
    "pq.write_table(table, preprocessed_file_path)\n",
    "\n",
    "# Optional: Display the schema and preview of the preprocessed dataset\n",
    "print(\"Preprocessed Schema:\")\n",
    "print(table.schema)\n",
    "print(\"Preview of the Preprocessed Dataset:\")\n",
    "print(table.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
